{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swap1984/GloVe-Embeddings-NLP-Vectorization-Pipeline/blob/main/GloVe_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMNhuBza4ton"
      },
      "source": [
        "#**Glove Embedding method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvG6d7_Z44MQ"
      },
      "source": [
        "**What Are GloVe Embeddings?**\n",
        "\n",
        "GloVe (Global Vectors for Word Representation) is a **popular pre-trained word embedding technique developed by Stanford.** It is based on the idea that the meaning of a word is captured by the co-occurrence probability of that word with other words in a large corpus. **GloVe embeddings are trained on a global word co-occurrence matrix, which gives them the ability to capture both the local context and global semantics.**\n",
        "\n",
        "**Advantages of GloVe Embeddings:**\n",
        "\n",
        "Efficient Pre-trained Embeddings:\n",
        "You can load pre-trained embeddings to save time, rather than training from scratch.\n",
        "\n",
        "Captures Word Meaning Well:\n",
        "GloVe embeddings capture semantic relationships, e.g., the vector for \"king\" – \"man\" + \"woman\" is close to \"queen.\"\n",
        "\n",
        "Available in Multiple Sizes:\n",
        "Pre-trained models are available with different dimensions (e.g., 50D, 100D, 300D), allowing you to choose based on your requirements.\n",
        "\n",
        "Handles Context:\n",
        "While not as powerful as contextual embeddings (like BERT), GloVe captures more global context than simple word embeddings like Word2Vec.\n",
        "\n",
        "**Disadvantages of GloVe Embeddings:**\n",
        "\n",
        "Static Embeddings:\n",
        "GloVe assigns the same vector to a word, irrespective of the context. For example, the word \"bank\" will have the same vector whether referring to a financial institution or a riverbank.\n",
        "\n",
        "Pre-trained on Specific Corpora:\n",
        " The pre-trained GloVe embeddings are trained on certain corpora (e.g., Wikipedia, Common Crawl), which may not always match your dataset's vocabulary.\n",
        "\n",
        "Cannot Handle Out-of-Vocabulary (OOV) Words:\n",
        "Words that were not present in the training data are not included in the pre-trained GloVe embeddings, making them harder to handle for domain-specific text.\n",
        "\n",
        "**Applications of GloVe Embeddings:**\n",
        "\n",
        "Text Classification:\n",
        "Word embeddings from GloVe can be used as input features for text classifiers.\n",
        "\n",
        "Semantic Search:\n",
        "Use embeddings to compute similarity between queries and documents.\n",
        "\n",
        "Named Entity Recognition (NER):\n",
        " Word embeddings help in recognizing entities by capturing semantic relationships between words.\n",
        "\n",
        "Sentiment Analysis:\n",
        " Pre-trained embeddings improve the performance of sentiment classifiers by capturing meaning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Qb7Po_8zyb"
      },
      "source": [
        "# **GloVe Embeddings Pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "E9SkRmHk36YS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb77e6c-2d6d-4c82-e7c7-8e983206735d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# initialising the libraries\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  # Ensure you have NLTK installed\n",
        "import string\n",
        "nltk.download('punkt')  # Download the punkt tokenizer\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4hnZ4zPH_FDY"
      },
      "outputs": [],
      "source": [
        "# using a paragraph as input . data =\"\"   \"\"\n",
        "data = \"\"\"Yes, life is full, there is life even underground,” he began again. “You wouldn’t believe, Alexey, how I want to live now, what a thirst for existence and consciousness has sprung up in me within these peeling walls… And what is suffering? I am not afraid of it, even if it were beyond reckoning. I am not afraid of it now. I was afraid of it before… And I seem to have such strength in me now, that I think I could stand anything, any suffering, only to be able to say and to repeat to myself every moment, ‘I exist.’ In thousands of agonies — I exist. I’m tormented on the rack — but I exist! Though I sit alone on a pillar — I exist! I see the sun, and if I don’t see the sun, I know it’s there. And there’s a whole life in that, in knowing that the sun is there.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2w4VMCBZ3E",
        "outputId": "3bd910cc-8386-4c4b-dcfd-5133ac605bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-20 08:08:07--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-11-20 08:08:07--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-11-20 08:08:08--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         42%[=======>            ] 346.33M  5.03MB/s    eta 91s    ^C\n"
          ]
        }
      ],
      "source": [
        "# Download the GloVe 6B vectors (around 812Mb)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mluabYT9C6Xz",
        "outputId": "875c4827-e806-4f02-d4e5-ab8d1265b5e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "# Unzip the file\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kM3rabTR9IXO"
      },
      "outputs": [],
      "source": [
        "# Define the file path for GloVe pre-trained embeddings\n",
        "glove_file = r'/content/glove.6B.200d.txt'  # Example for 200-dimensional embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                continue\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            try:\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return embeddings_index\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jok-rAUsKPmK"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert tokens to embeddings\n",
        "def text_to_embeddings(tokens, glove_embeddings):\n",
        "    embeddings = []\n",
        "    for word in tokens:\n",
        "        if word in glove_embeddings:\n",
        "            embeddings.append(glove_embeddings[word])\n",
        "        else:\n",
        "            # Append a zero vector of the appropriate dimension if the word is not found\n",
        "            embeddings.append(np.zeros((200,)))  # Adjust if using different dimensions\n",
        "    return np.array(embeddings,dtype='float32')"
      ],
      "metadata": {
        "id": "M3JyDhEgOACZ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "glove_file_path = '/content/glove.6B.200d.txt'  # Replace with the path to your GloVe file\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)"
      ],
      "metadata": {
        "id": "idu0rNTUKfdE"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input data\n",
        "tokens = word_tokenize(data.lower())  # Tokenize and convert to lowercase\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjPv2-5WL48F",
        "outputId": "662131d6-5912-4ab1-b914-679d474b3c9b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['yes', ',', 'life', 'is', 'full', ',', 'there', 'is', 'life', 'even', 'underground', ',', '”', 'he', 'began', 'again', '.', '“', 'you', 'wouldn', '’', 't', 'believe', ',', 'alexey', ',', 'how', 'i', 'want', 'to', 'live', 'now', ',', 'what', 'a', 'thirst', 'for', 'existence', 'and', 'consciousness', 'has', 'sprung', 'up', 'in', 'me', 'within', 'these', 'peeling', 'walls…', 'and', 'what', 'is', 'suffering', '?', 'i', 'am', 'not', 'afraid', 'of', 'it', ',', 'even', 'if', 'it', 'were', 'beyond', 'reckoning', '.', 'i', 'am', 'not', 'afraid', 'of', 'it', 'now', '.', 'i', 'was', 'afraid', 'of', 'it', 'before…', 'and', 'i', 'seem', 'to', 'have', 'such', 'strength', 'in', 'me', 'now', ',', 'that', 'i', 'think', 'i', 'could', 'stand', 'anything', ',', 'any', 'suffering', ',', 'only', 'to', 'be', 'able', 'to', 'say', 'and', 'to', 'repeat', 'to', 'myself', 'every', 'moment', ',', '‘', 'i', 'exist.', '’', 'in', 'thousands', 'of', 'agonies', '—', 'i', 'exist', '.', 'i', '’', 'm', 'tormented', 'on', 'the', 'rack', '—', 'but', 'i', 'exist', '!', 'though', 'i', 'sit', 'alone', 'on', 'a', 'pillar', '—', 'i', 'exist', '!', 'i', 'see', 'the', 'sun', ',', 'and', 'if', 'i', 'don', '’', 't', 'see', 'the', 'sun', ',', 'i', 'know', 'it', '’', 's', 'there', '.', 'and', 'there', '’', 's', 'a', 'whole', 'life', 'in', 'that', ',', 'in', 'knowing', 'that', 'the', 'sun', 'is', 'there', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to embeddings\n",
        "embeddings_array = text_to_embeddings(tokens, glove_embeddings)"
      ],
      "metadata": {
        "id": "Oiil5IlAOQjy"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the embeddings\n",
        "print(f\"Shape of the embeddings array: {embeddings_array.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOtRB9OhS7nB",
        "outputId": "04001f30-6e6a-4572-daec-00435af36241"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the embeddings array: (193, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first 5 embeddings for the first 5 tokens of the data\n",
        "print(embeddings_array[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFw8UXBZTB4w",
        "outputId": "c1b73290-e9eb-47b4-8468-2619d2b02167"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.2042e-01  2.4924e-01  1.7393e-01 -1.6275e-02 -3.0916e-01  2.6610e-01\n",
            "  -7.4482e-01  8.3853e-02  3.2016e-01  6.5026e-01 -3.0558e-01  1.6842e-02\n",
            "   4.8508e-02 -2.9494e-01 -7.8175e-01  4.1424e-01 -2.1490e-01  5.3132e-01\n",
            "   4.8179e-01  1.5397e-01  3.0971e-01  1.4896e+00 -8.2240e-02  3.2086e-01\n",
            "   2.8866e-02 -2.9613e-01 -1.3558e-01 -7.4538e-02  2.3826e-01 -5.3784e-01\n",
            "  -4.1572e-01 -1.3477e-01  1.5444e-01 -1.2929e-01 -3.4317e-01 -1.5537e-01\n",
            "   3.3685e-02 -3.5089e-01 -2.2403e-01  4.9218e-01 -1.4750e-01  3.4514e-03\n",
            "  -2.4207e-01 -1.0924e-01  5.7027e-03  1.2135e-01  6.0403e-01 -1.7868e-01\n",
            "  -3.9604e-01  5.6209e-02  5.1824e-01 -3.9824e-01  1.3078e-01  1.3691e-01\n",
            "  -1.5311e-01 -4.9687e-02  4.9536e-02 -3.3107e-02 -1.2196e-02 -3.2973e-01\n",
            "   2.5021e-01 -3.2097e-01  1.4448e-01  5.5550e-02  6.1817e-02 -7.5844e-02\n",
            "  -1.9434e-01  6.2882e-01  8.3724e-01  1.1561e-01  1.9471e-01  3.9750e-03\n",
            "   1.7576e-02 -3.6561e-01 -4.1066e-01 -4.7927e-01 -5.2158e-01  2.1736e-01\n",
            "  -5.6578e-01 -1.5558e-01  3.5471e-01 -7.0188e-03 -1.9452e-03  1.7939e-01\n",
            "  -3.5319e-01 -2.5795e-01 -3.1974e-01 -4.4591e-02  1.2496e-01 -9.3906e-01\n",
            "  -1.4558e-01 -2.3784e-01  3.3792e-01 -2.5189e-01 -2.2839e-01  7.8948e-02\n",
            "   3.5851e-01  2.1203e-02 -8.1422e-01  2.3170e-01  4.7906e-01  9.6946e-02\n",
            "   2.3977e-01 -3.8182e-01  3.7931e-01 -3.6341e-02 -3.6294e-01  1.3091e+00\n",
            "  -4.8050e-02  3.7740e-01  1.4168e-02  1.2427e-01  8.0578e-02  2.1790e-02\n",
            "  -5.7981e-01  1.9524e-02 -8.1258e-02 -2.1899e-02 -2.8081e-01  2.1055e-01\n",
            "   1.9828e-01 -8.3682e-02  1.4192e-01  6.8194e-02  2.0974e-01 -4.5658e-01\n",
            "  -2.3901e-02 -6.1730e-01  4.8950e-01 -3.0713e-01 -1.5431e-01 -1.9995e-01\n",
            "  -3.6085e-02  1.7406e-01 -7.2787e-03 -1.0505e-01 -1.4231e-01 -2.1916e-01\n",
            "   1.7860e-01 -2.7257e-02  1.0534e-01  7.3802e-02  1.2930e-01 -4.1754e-01\n",
            "   1.2693e+00  1.4427e-01  3.7295e-01 -4.2493e-01 -6.9197e-01  8.0780e-02\n",
            "   1.4119e-02  6.7835e-01  1.6693e-01  2.6872e-01  3.2128e-01  4.6269e-03\n",
            "  -2.1230e-02 -1.3293e-01  1.4867e-01  3.6675e-02 -9.8082e-02 -2.4252e-01\n",
            "  -5.5073e-02 -3.3584e-01 -3.2442e-01 -5.4760e-01  2.1091e-01  1.2983e-01\n",
            "  -1.4290e-01 -6.0260e-01 -7.6548e-02  4.5740e-01  2.7267e-01  1.2926e-01\n",
            "  -2.1670e-01 -1.5686e-01 -4.6502e-01  1.5973e-01 -1.5449e-01  3.6390e-01\n",
            "   9.7274e-01  1.1432e-01 -3.3772e-02  2.3939e-02 -2.3805e-01 -3.1564e-01\n",
            "   5.0120e-01  4.9548e-02  2.0645e-01 -6.6727e-01 -2.3230e-01  1.1915e-01\n",
            "   2.2961e-01 -4.1109e-01  5.0235e-02 -3.8869e-01 -9.6837e-02 -8.8502e-02\n",
            "  -1.9784e-01  3.8890e-01]\n",
            " [ 1.7651e-01  2.9208e-01 -2.0768e-03 -3.7523e-01  4.9139e-03  2.3979e-01\n",
            "  -2.8893e-01 -1.4643e-02 -1.0993e-01  1.5592e-01  2.0627e-01  4.7675e-01\n",
            "   9.9907e-02 -1.4058e-01  2.1114e-01  1.2126e-01 -3.1831e-01 -8.9433e-02\n",
            "  -9.0553e-02 -3.1962e-01  2.1319e-01  2.4844e+00 -7.7521e-02 -8.4279e-02\n",
            "   2.0186e-01  2.6084e-01 -4.0411e-01 -1.9127e-01  2.4715e-01  2.2394e-01\n",
            "  -6.3437e-02  2.0379e-01 -1.8463e-01 -8.8413e-02  2.4169e-02 -2.8769e-01\n",
            "  -6.1246e-01 -1.2683e-01 -8.8273e-02  1.8331e-01 -5.3161e-01 -1.9970e-01\n",
            "  -2.6703e-01  1.5312e-01 -1.5239e-02 -8.2844e-02  4.7856e-01 -2.9612e-01\n",
            "   1.1168e-01 -2.5790e-02 -1.1697e-02  1.9923e-01 -1.4267e-01  6.6250e-01\n",
            "  -5.1739e-02 -1.6938e-01 -1.5635e-01  9.2806e-02  3.2548e-01  1.1724e-01\n",
            "   2.8788e-01 -6.0651e-02 -1.4153e-01  1.6668e-01  2.6861e-01 -3.1001e-02\n",
            "  -3.9665e-01  3.5304e-01  2.3850e-01  1.2388e-01  4.5698e-01 -1.2559e-01\n",
            "  -1.2804e-01  3.7449e-01  2.4460e-01  2.3073e-01  2.0808e-01  5.1258e-02\n",
            "  -2.1816e-01 -3.6409e-02 -3.8800e-02 -4.2487e-02 -3.0779e-01 -2.5449e-02\n",
            "   2.2532e-01  4.5538e-02 -4.8934e-01 -1.3988e-01  1.7394e-01 -4.6137e-01\n",
            "  -2.6555e-01  1.5473e-01  6.3816e-02 -1.7022e-01 -1.5762e-01  7.5765e-02\n",
            "   1.2151e-01 -4.9340e-01 -1.0909e-01  3.4487e-02  2.9947e-01  1.8690e-02\n",
            "  -1.6534e-01  1.6679e-02  1.6341e-01 -2.7418e-01  7.7797e-02  1.4023e+00\n",
            "   2.5275e-02  9.4725e-02 -4.0735e-02 -1.0642e-01  2.3364e-02  7.9143e-02\n",
            "  -1.6615e-01 -2.3013e-01 -1.4071e-01  4.0159e-01 -3.4951e-01  1.8545e-02\n",
            "   2.2434e-01  7.6922e-01  2.4722e-01  1.4936e-01  4.2368e-01 -7.2059e-01\n",
            "  -3.8541e-02  1.5522e-01  3.3596e-01 -4.3077e-01 -2.6925e-02 -3.7733e-01\n",
            "   2.4271e-01 -4.6495e-01  4.5783e-01  2.3693e-01  7.9361e-02 -3.2244e-01\n",
            "  -4.2434e-01 -1.1138e-01  5.5426e-01  8.5153e-02 -2.0581e-02 -4.6386e-02\n",
            "   1.2467e+00  1.3177e-01  6.7092e-02 -5.7780e-01  1.3586e-02 -7.1274e-02\n",
            "   1.7311e-02  8.9781e-02  1.9857e-01 -3.2205e-02  6.4843e-01 -2.3797e-01\n",
            "  -1.9676e-01  2.0203e-01  2.1074e-01 -5.0347e-01  2.6823e-02 -4.5444e-02\n",
            "  -2.2642e-01 -1.9977e-01 -1.2138e-01  1.6941e-01  6.1998e-02  4.2631e-01\n",
            "  -8.8383e-02  4.5756e-01  7.7774e-02  6.1342e-02  4.5710e-01 -1.7787e-01\n",
            "  -1.4597e-01  3.2654e-01  2.4430e-03 -1.1886e-01  1.0081e-01 -2.0011e-02\n",
            "   1.0366e+00 -3.9814e-01 -6.8180e-01  2.3685e-01 -2.0396e-01 -1.7668e-01\n",
            "  -3.1385e-01  1.4834e-01 -5.2187e-02  6.1300e-02 -3.2582e-01  1.9153e-01\n",
            "  -1.5469e-01 -1.4679e-01  4.6971e-02  3.2325e-02 -2.2006e-01 -2.0774e-01\n",
            "  -2.3189e-01 -1.0814e-01]\n",
            " [ 3.4098e-01  4.1888e-01 -3.1878e-01  3.1399e-02  4.7223e-02  2.4906e-01\n",
            "  -7.1631e-01  5.2904e-02 -3.4208e-03  2.1647e-01 -3.0463e-02  8.9968e-01\n",
            "   3.5829e-01 -6.6277e-02  8.1989e-02  3.9360e-01  9.9335e-02  7.3747e-01\n",
            "   3.2178e-01 -2.2824e-01  3.9632e-01  2.9324e+00  2.0929e-01 -2.6525e-01\n",
            "   1.2720e-01  8.7867e-02 -3.5642e-03 -5.1395e-02 -3.8543e-01  2.6079e-01\n",
            "   5.8987e-02 -3.2565e-02  3.4674e-01 -6.5351e-02 -1.2305e-01  1.8915e-02\n",
            "  -1.9741e-01  2.4483e-01 -7.6178e-02  1.4100e-01 -9.6150e-01 -1.7128e-02\n",
            "  -1.9702e-01  5.8651e-01 -1.0661e-01  1.0472e-01  6.7752e-01  4.9478e-01\n",
            "   1.1170e-01  1.8718e-02 -2.6009e-01  4.9720e-01 -1.5728e-01  6.0994e-01\n",
            "   1.8953e-01  1.1996e-01  2.7827e-01 -1.5831e-02  3.9711e-02 -3.7136e-01\n",
            "  -2.8909e-01 -3.2909e-01 -7.1628e-01  5.6670e-01 -2.3132e-01 -1.4021e-01\n",
            "   5.5793e-01  3.1357e-01 -5.7165e-01  3.4597e-01  7.1454e-01 -1.7147e-01\n",
            "   8.9315e-02  8.1979e-01  2.3206e-01  1.9429e-01 -4.4881e-01 -2.8906e-01\n",
            "  -1.5707e-02  1.4476e-01 -1.4431e-02  1.1904e-01 -6.6498e-01 -3.8981e-02\n",
            "   1.7994e-01  9.7028e-02 -3.4582e-01  6.7696e-03  4.7810e-01 -6.0951e-01\n",
            "   6.4572e-01  7.3228e-04  3.7128e-01  1.7062e-01 -2.6385e-01 -4.2804e-01\n",
            "  -2.2658e-01 -1.8425e-01  5.2100e-03  1.8815e-01  1.2560e-01  6.0294e-02\n",
            "  -2.0199e-01  2.6285e-02  2.4987e-02 -7.4668e-01  2.0404e-01  7.0431e-01\n",
            "  -2.0175e-01 -2.1715e-01  1.6100e-01 -1.3599e-01  1.9623e-01  2.2066e-02\n",
            "  -3.3890e-01 -1.9406e-01 -1.5501e-01  5.4023e-01 -6.1807e-01 -3.9910e-01\n",
            "  -1.7745e-01  2.6452e-01 -1.2936e-01 -1.9009e-01 -8.3096e-02 -3.2137e-01\n",
            "   5.8294e-01  1.6789e-01  4.7591e-02 -9.2068e-01 -3.5928e-01 -1.0102e-01\n",
            "   9.4616e-02  5.1867e-02  8.9359e-02  1.6542e-01  7.5411e-01 -1.1097e-01\n",
            "  -2.3721e-01 -1.3093e-01  5.3672e-01  3.0311e-01 -8.0717e-02 -1.6100e-01\n",
            "   1.4385e+00  1.9252e-02 -1.5350e-01  1.8693e-02  2.7981e-01  4.8011e-01\n",
            "   7.4099e-02 -1.0303e-01 -2.4328e-01  6.9507e-02  3.4220e-01 -1.9133e-01\n",
            "   5.9109e-02 -2.8913e-02 -1.6200e-01  3.7345e-01  1.4532e-01  7.4352e-01\n",
            "   9.0516e-02 -3.8116e-01  1.1042e-01 -2.5854e-01 -4.0525e-01 -3.6723e-02\n",
            "  -1.8737e-01  3.2509e-01 -2.7669e-01  3.1600e-01  3.0019e-01  5.3721e-01\n",
            "  -4.3686e-01  1.5107e-01 -2.3496e-01 -9.7297e-02  4.1126e-01  6.4296e-01\n",
            "   1.2471e+00 -1.7064e-01 -3.7883e-01  3.1271e-02 -1.9095e-01  5.0932e-01\n",
            "   4.7580e-02 -9.3402e-02  6.1584e-01  2.1330e-01 -2.4400e-01  5.7573e-02\n",
            "  -1.7811e-01 -1.7605e-01  1.4003e-01 -6.9920e-01  2.8465e-01  7.2919e-01\n",
            "   3.8914e-01  3.7687e-01]\n",
            " [ 3.2928e-01  2.5526e-01  2.6753e-01 -8.4809e-02  2.9764e-01  6.2339e-02\n",
            "  -1.5475e-01  1.7784e-01  3.2328e-01 -9.2752e-01  1.5194e-01  1.6324e-01\n",
            "  -1.0428e-01 -2.6464e-02  6.5971e-01  1.4782e-01  3.8623e-01  2.5169e-01\n",
            "   1.2610e-01 -4.3138e-01  2.8092e-01  3.1604e+00 -1.7565e-01 -3.2247e-03\n",
            "   6.4389e-01 -3.9697e-01  1.8975e-01  3.7999e-01 -7.9175e-02 -1.4781e-01\n",
            "  -7.2965e-02  5.7247e-02 -4.2314e-01  4.5080e-01 -9.7386e-02 -4.7587e-01\n",
            "  -9.6599e-01 -7.5595e-01 -3.3932e-02 -7.0886e-02 -4.4828e-01 -5.2094e-01\n",
            "  -1.8230e-01  1.8582e-01 -7.4273e-02 -1.7871e-02  1.6742e-01  1.5459e-02\n",
            "   3.0290e-01 -1.2580e-01  3.2418e-01 -3.1263e-01 -7.6832e-02  5.1959e-02\n",
            "   2.7242e-01 -1.8285e-01 -3.6479e-01 -6.3562e-01 -2.1685e-01  3.5812e-02\n",
            "   1.2485e-01  3.7268e-01 -1.6976e-01 -9.4146e-02 -1.6412e-01 -1.0728e-01\n",
            "   3.7866e-02  1.1750e-01 -1.5533e-01  3.4062e-01  5.8848e-01  3.8992e-01\n",
            "  -5.4839e-01  8.5013e-01 -8.3728e-01  1.5482e-01 -3.7191e-01 -6.5409e-01\n",
            "  -2.7631e-01 -2.5224e-02  7.5732e-02 -2.3904e-01 -1.8311e-01 -8.4571e-02\n",
            "   1.5492e-01 -1.6317e-01 -2.6499e-01  5.6831e-02  8.8287e-01 -4.7655e-01\n",
            "   2.5131e-01 -9.3160e-02  3.4377e-01 -3.5863e-01 -2.2855e-01  1.1918e-01\n",
            "   2.9661e-01 -2.5360e-01  4.9002e-02 -2.1234e-01  1.6237e-01  5.3871e-01\n",
            "   3.5344e-02  3.9293e-01 -2.9673e-01 -7.2556e-01 -2.7431e-01  1.3469e+00\n",
            "  -1.9218e-01  5.0534e-01  2.8451e-02 -3.2206e-01  9.6035e-02 -8.3551e-03\n",
            "  -1.3107e-02 -3.2444e-01 -1.0163e-01  3.1755e-02 -6.3196e-01 -2.1541e-01\n",
            "  -3.5609e-02  3.1259e-01  2.3988e-01 -1.9056e-01 -1.3086e-01 -1.2644e-01\n",
            "   4.8795e-01 -1.3492e-01 -4.1967e-01  1.5904e-01 -2.7921e-01 -1.7258e-02\n",
            "   2.9370e-01  6.7436e-02  8.5052e-02  9.9394e-02 -5.5281e-03  9.4985e-02\n",
            "   1.1167e-01  1.9749e-01  2.5230e-01  3.2205e-01  4.2778e-01 -3.5180e-02\n",
            "   1.3291e+00  5.2610e-03  2.6769e-01 -4.6168e-01  1.1250e-01  1.0111e-01\n",
            "  -3.1174e-01  5.4580e-01 -3.7363e-01 -2.6133e-02  9.9566e-01 -1.5827e-01\n",
            "  -2.6202e-01  1.7324e-01  6.0104e-02 -4.8004e-01  2.3841e-01 -2.1495e-01\n",
            "   7.7693e-02 -8.9078e-02  1.2985e-01 -1.7400e-01 -5.7151e-02  4.8207e-01\n",
            "  -1.4668e-01  2.6739e-01 -3.3366e-01  3.2552e-01  6.2520e-01 -3.0905e-01\n",
            "   8.7737e-02 -1.7204e-01  2.8246e-01 -3.7268e-02  1.6007e-01  3.0031e-01\n",
            "   1.4061e+00 -3.2169e-01 -2.5792e-02  3.7175e-02  2.6222e-02 -2.7671e-01\n",
            "   5.1688e-02 -5.8734e-02 -2.3223e-01 -1.0529e-01 -4.0318e-01 -2.2161e-01\n",
            "   6.0587e-02  9.1321e-02 -2.1363e-01  7.1634e-02 -2.1331e-01  7.4621e-02\n",
            "   1.2001e-02 -2.1952e-01]\n",
            " [ 2.9667e-01 -1.9041e-01 -1.6734e-01 -2.3067e-01  3.5460e-02 -1.5565e-01\n",
            "   1.0894e-01 -1.8320e-01 -7.8841e-02  3.1029e-01 -2.6727e-01  2.6272e-01\n",
            "   2.6124e-01 -3.3728e-01  4.2254e-01 -4.8555e-01  1.3919e-01  3.5297e-01\n",
            "  -1.9247e-01 -2.2333e-01  4.9861e-01  2.7357e+00  1.3011e-01 -5.0954e-01\n",
            "   4.5014e-01 -8.1410e-02 -2.3534e-01  1.0371e-02  4.3054e-01 -2.1223e-01\n",
            "  -1.9638e-01 -3.1434e-01 -1.2297e-01 -5.6133e-01 -9.0384e-02 -3.3263e-02\n",
            "  -3.2137e-01 -5.4950e-01 -1.0027e-02  2.3146e-01  1.0421e-02 -1.3659e-02\n",
            "  -7.6713e-03 -1.3176e-01  1.0317e-02  3.7078e-01  4.6923e-01  5.9423e-02\n",
            "   4.1123e-01  1.0023e-02 -9.0452e-02 -4.0759e-01 -1.1723e-01  3.0375e-01\n",
            "   2.5175e-01  1.5379e-01  1.5170e-01 -2.0262e-02 -1.7740e-01 -1.8909e-01\n",
            "   6.7966e-01 -6.6682e-01 -4.9400e-01 -5.9751e-01  1.0683e-01  1.3225e-01\n",
            "   2.7032e-01 -1.0069e-01  2.5423e-01  1.3450e-01  5.0991e-01  2.7086e-01\n",
            "   3.4327e-01  2.3083e-02 -1.2101e-01  1.7861e-01 -1.9951e-01 -2.3792e-01\n",
            "   4.3102e-01  5.5752e-02  1.6033e-01 -3.4647e-01 -2.1565e-01 -7.6750e-02\n",
            "   1.7376e-01 -2.6902e-01  2.2254e-01 -3.2923e-01  7.6815e-01 -7.1194e-01\n",
            "   3.4376e-01  2.2850e-01  4.1817e-01  1.3746e-02  2.8038e-01 -3.1916e-01\n",
            "   3.1060e-01  2.9356e-01 -2.8953e-01  1.0565e-01  2.7560e-01 -3.7460e-01\n",
            "   6.0083e-01  1.7285e-04 -2.1326e-01  3.0623e-02  9.0689e-02  1.0639e+00\n",
            "  -4.1776e-01 -2.2014e-01 -3.7017e-01 -4.8046e-01  6.7010e-02 -7.5316e-02\n",
            "   8.5481e-02 -4.0933e-01  2.6082e-01 -2.5018e-01  9.1720e-02 -1.2541e-01\n",
            "  -4.6252e-01  2.0970e-01 -2.3605e-01  2.8665e-01  3.5655e-02 -3.4872e-01\n",
            "   4.5211e-01 -1.3086e-01  3.5587e-01  6.8709e-02 -4.8611e-02 -7.7557e-03\n",
            "   9.3124e-02 -1.6243e-01  3.5604e-01  2.6036e-01  1.2996e-01 -5.3751e-01\n",
            "   5.2925e-01  1.7803e-01  4.2875e-01 -3.2468e-01  2.8189e-01  3.7518e-01\n",
            "   1.2964e+00  3.4064e-02 -3.0608e-01 -1.6244e-01 -1.8960e-01 -1.9614e-01\n",
            "   3.5166e-01 -2.4670e-01 -2.1728e-01  1.9992e-01  2.7304e-02 -1.4604e-01\n",
            "  -3.4660e-01 -1.2907e-02  6.7934e-02 -3.3142e-01  2.1655e-01 -2.3391e-01\n",
            "   2.1805e-01  4.7136e-02  1.8012e-01  1.2672e-01 -5.3111e-01 -4.8423e-01\n",
            "  -5.1849e-01  6.2180e-01  2.8892e-02  6.2254e-02  2.1884e-01  4.6848e-01\n",
            "  -3.4412e-02  1.1361e-01 -6.7913e-01 -3.8568e-01 -3.0395e-01 -4.9260e-02\n",
            "   1.3075e+00 -5.2156e-01 -3.1272e-01 -2.5363e-01 -1.0971e-01 -2.0867e-01\n",
            "  -4.4347e-01  1.1496e-01  1.4502e-01  3.9719e-01 -1.6284e-01  3.0485e-01\n",
            "  -1.3438e-01 -6.4691e-01  1.1908e-01 -5.3820e-01  5.5905e-01 -4.4982e-02\n",
            "  -5.2480e-01  4.3902e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve the embedding\n",
        "def get_embedding(token, glove_model, embedding_dim=300):\n",
        "    return glove_model.get(token, np.zeros(embedding_dim))\n"
      ],
      "metadata": {
        "id": "m0A-ZJr6Wl2C"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve embeddings for the tokens\n",
        "embeddings_array = [get_embedding(token, glove_embeddings) for token in tokens]"
      ],
      "metadata": {
        "id": "ASSGUqZ_WssX"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the output\n",
        "for token, embedding in zip(tokens, embeddings_array):\n",
        "    print(f\"Token: '{token}', Embedding: {embedding[:5]}\")  # Print first 5 dimensions of the embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waJ6kDNeWxhq",
        "outputId": "0fd12a2a-e4ab-44fa-e6d7-306d75b4944b",
        "collapsed": true
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: 'yes', Embedding: [ 0.22042   0.24924   0.17393  -0.016275 -0.30916 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'life', Embedding: [ 0.34098   0.41888  -0.31878   0.031399  0.047223]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'full', Embedding: [ 0.29667 -0.19041 -0.16734 -0.23067  0.03546]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'life', Embedding: [ 0.34098   0.41888  -0.31878   0.031399  0.047223]\n",
            "Token: 'even', Embedding: [ 0.44802   0.16025  -0.23372  -0.054205 -0.067149]\n",
            "Token: 'underground', Embedding: [-0.20773 -0.26716 -0.64454 -0.14187  0.3224 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: '”', Embedding: [ 0.10706   0.25534   0.036386 -0.01086  -0.1454  ]\n",
            "Token: 'he', Embedding: [ 0.10278  -0.037982 -0.34679  -0.20236  -0.10104 ]\n",
            "Token: 'began', Embedding: [-0.68062   0.057562 -0.48837  -0.6388   -0.39483 ]\n",
            "Token: 'again', Embedding: [-0.016971  0.39541  -0.50494   0.012802 -0.36807 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: '“', Embedding: [ 0.20132   0.21419  -0.044341  0.057602 -0.19705 ]\n",
            "Token: 'you', Embedding: [ 0.85395   0.57146  -0.023652 -0.11047  -0.1275  ]\n",
            "Token: 'wouldn', Embedding: [-0.11372 -0.25854  0.1247   0.59396  0.39038]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 't', Embedding: [ 0.42596 -0.18836 -0.65115  0.72988  0.86216]\n",
            "Token: 'believe', Embedding: [-0.095714  0.17591   0.17628  -0.10767  -0.073885]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'alexey', Embedding: [0.54669  0.58661  0.43686  0.097392 0.14128 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'how', Embedding: [ 0.18146   0.2663    0.054955  0.23884  -0.047094]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'want', Embedding: [ 0.43249   0.53471  -0.018324  0.15637   0.066969]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'live', Embedding: [ 0.28944  0.13113  0.11904 -0.15172 -0.73016]\n",
            "Token: 'now', Embedding: [ 0.17821  -0.043552 -0.047699  0.20681   0.18358 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'what', Embedding: [ 0.39396    0.44185   -0.0042279 -0.044507  -0.13598  ]\n",
            "Token: 'a', Embedding: [ 0.24169 -0.34534 -0.22307 -1.2907   0.25285]\n",
            "Token: 'thirst', Embedding: [0.57785  0.31791  0.066497 0.18393  0.1795  ]\n",
            "Token: 'for', Embedding: [ 0.29117   0.86093  -0.66616  -0.13595   0.057122]\n",
            "Token: 'existence', Embedding: [ 0.29411  -0.5611    0.19299   0.045764 -0.38261 ]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'consciousness', Embedding: [-0.94552  0.3193   0.262    0.41797  0.25382]\n",
            "Token: 'has', Embedding: [-0.30666  -0.08218  -0.32038  -0.11622   0.093608]\n",
            "Token: 'sprung', Embedding: [-0.002931  0.085132 -0.42228  -0.15877  -0.095795]\n",
            "Token: 'up', Embedding: [ 0.32335   0.10116  -0.7182   -0.098222  0.10265 ]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'me', Embedding: [ 0.34605  0.33953 -0.01154  0.11595 -0.27834]\n",
            "Token: 'within', Embedding: [-0.15887  -0.014129  0.41397  -0.098472 -0.086403]\n",
            "Token: 'these', Embedding: [ 0.2408   0.46328 -0.18936 -0.16177  0.20385]\n",
            "Token: 'peeling', Embedding: [-0.54873 -0.4133  -0.10959  0.14457  0.51581]\n",
            "Token: 'walls…', Embedding: [0. 0. 0. 0. 0.]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'what', Embedding: [ 0.39396    0.44185   -0.0042279 -0.044507  -0.13598  ]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'suffering', Embedding: [-0.31963  -0.18421  -0.20885   0.017626 -0.35361 ]\n",
            "Token: '?', Embedding: [ 0.39111   0.40186  -0.15055  -0.035758 -0.27055 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'am', Embedding: [-0.26604  0.4732   0.31872 -0.46331 -0.25759]\n",
            "Token: 'not', Embedding: [ 0.34303   0.4082   -0.023317 -0.36093   0.0526  ]\n",
            "Token: 'afraid', Embedding: [ 0.32403  -0.17814  -0.019409  0.056288 -0.43115 ]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'even', Embedding: [ 0.44802   0.16025  -0.23372  -0.054205 -0.067149]\n",
            "Token: 'if', Embedding: [0.56699  0.57946  0.18112  0.17006  0.074038]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: 'were', Embedding: [-0.29716  -0.025477 -0.43389  -0.52616  -0.14354 ]\n",
            "Token: 'beyond', Embedding: [ 0.19766  -0.11626  -0.23853  -0.046909 -0.12131 ]\n",
            "Token: 'reckoning', Embedding: [ 0.15398    0.15808    0.11411    0.0027507 -0.27107  ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'am', Embedding: [-0.26604  0.4732   0.31872 -0.46331 -0.25759]\n",
            "Token: 'not', Embedding: [ 0.34303   0.4082   -0.023317 -0.36093   0.0526  ]\n",
            "Token: 'afraid', Embedding: [ 0.32403  -0.17814  -0.019409  0.056288 -0.43115 ]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: 'now', Embedding: [ 0.17821  -0.043552 -0.047699  0.20681   0.18358 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'was', Embedding: [-0.1996  -0.27529 -0.21257 -0.71516 -0.1549 ]\n",
            "Token: 'afraid', Embedding: [ 0.32403  -0.17814  -0.019409  0.056288 -0.43115 ]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: 'before…', Embedding: [0. 0. 0. 0. 0.]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'seem', Embedding: [ 0.46434  0.16059 -0.14039 -0.28814  0.24309]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'have', Embedding: [-0.243    -0.014334 -0.38246  -0.21259  -0.095778]\n",
            "Token: 'such', Embedding: [ 0.42784  0.64209 -0.2957  -0.34537  0.39223]\n",
            "Token: 'strength', Embedding: [ 0.21762  -0.053931  0.059835 -0.036046  0.21309 ]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'me', Embedding: [ 0.34605  0.33953 -0.01154  0.11595 -0.27834]\n",
            "Token: 'now', Embedding: [ 0.17821  -0.043552 -0.047699  0.20681   0.18358 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'that', Embedding: [ 0.14805   0.10875  -0.036278 -0.15386   0.37181 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'think', Embedding: [ 0.31806   0.20393  -0.12459  -0.021774 -0.021626]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'could', Embedding: [ 0.15973  0.1416  -0.2158  -0.22323  0.35053]\n",
            "Token: 'stand', Embedding: [ 0.22787  0.30175 -0.40343  0.13181 -0.33456]\n",
            "Token: 'anything', Embedding: [ 0.42697  0.18069 -0.11546 -0.30937 -0.1883 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'any', Embedding: [ 0.63113  0.43183  0.23103 -0.64909  0.23744]\n",
            "Token: 'suffering', Embedding: [-0.31963  -0.18421  -0.20885   0.017626 -0.35361 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'only', Embedding: [ 0.16347   0.17487  -0.13832  -0.18389   0.086837]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'be', Embedding: [ 0.14336    0.32323   -0.0012141 -0.30418    0.032943 ]\n",
            "Token: 'able', Embedding: [ 0.42923  0.22089 -0.22135 -0.19993  0.23083]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'say', Embedding: [ 0.13206   -0.0024709 -0.030482  -0.055545   0.025463 ]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'repeat', Embedding: [ 0.48671   0.93464  -0.015418 -0.46056  -0.15226 ]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'myself', Embedding: [ 0.26019  0.4785  -0.53881  0.22078 -0.51154]\n",
            "Token: 'every', Embedding: [ 0.55499   0.67268  -0.081677 -0.30823  -0.082756]\n",
            "Token: 'moment', Embedding: [ 0.31521  0.54651 -0.13988 -0.52135 -0.72683]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: '‘', Embedding: [ 0.33057    0.0058394 -0.0026677  0.28709   -0.2346   ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist.', Embedding: [0. 0. 0. 0. 0.]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'thousands', Embedding: [ 0.16164 -0.44544 -0.20135  0.08019 -0.24991]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'agonies', Embedding: [-0.12813  -0.091163  0.15875   0.5773   -0.39179 ]\n",
            "Token: '—', Embedding: [ 0.23401  -0.003888 -0.015521  0.032232 -0.13994 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist', Embedding: [0.40714 0.26434 0.37526 0.01078 0.10304]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 'm', Embedding: [ 0.27704 -0.34803 -0.54378  0.2792   0.44356]\n",
            "Token: 'tormented', Embedding: [-0.087608 -0.27079   0.058618  0.59892  -0.22894 ]\n",
            "Token: 'on', Embedding: [-0.39374   0.55684  -0.35848  -0.67074   0.073665]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'rack', Embedding: [0.47955 0.77247 0.13851 0.62758 0.16891]\n",
            "Token: '—', Embedding: [ 0.23401  -0.003888 -0.015521  0.032232 -0.13994 ]\n",
            "Token: 'but', Embedding: [ 0.24438   0.053703 -0.23639  -0.25545   0.18797 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist', Embedding: [0.40714 0.26434 0.37526 0.01078 0.10304]\n",
            "Token: '!', Embedding: [ 0.50368  0.7717  -0.49917 -0.11844 -0.31343]\n",
            "Token: 'though', Embedding: [ 0.19866   -0.095063  -0.10798   -0.11013    0.0052079]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'sit', Embedding: [ 0.65503   0.27148  -0.13185  -0.085942 -0.28181 ]\n",
            "Token: 'alone', Embedding: [ 0.35012   0.23944  -0.18498   0.065594 -0.13079 ]\n",
            "Token: 'on', Embedding: [-0.39374   0.55684  -0.35848  -0.67074   0.073665]\n",
            "Token: 'a', Embedding: [ 0.24169 -0.34534 -0.22307 -1.2907   0.25285]\n",
            "Token: 'pillar', Embedding: [-0.098824 -0.22158  -0.31766   0.65825  -0.50977 ]\n",
            "Token: '—', Embedding: [ 0.23401  -0.003888 -0.015521  0.032232 -0.13994 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist', Embedding: [0.40714 0.26434 0.37526 0.01078 0.10304]\n",
            "Token: '!', Embedding: [ 0.50368  0.7717  -0.49917 -0.11844 -0.31343]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'see', Embedding: [ 0.57638   0.59676  -0.073165  0.11275   0.2183  ]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'sun', Embedding: [ 0.45253  0.35506  0.3146  -0.20477  0.66384]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'if', Embedding: [0.56699  0.57946  0.18112  0.17006  0.074038]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'don', Embedding: [-0.86746   0.47017  -0.50487  -0.35623  -0.064959]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 't', Embedding: [ 0.42596 -0.18836 -0.65115  0.72988  0.86216]\n",
            "Token: 'see', Embedding: [ 0.57638   0.59676  -0.073165  0.11275   0.2183  ]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'sun', Embedding: [ 0.45253  0.35506  0.3146  -0.20477  0.66384]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'know', Embedding: [ 0.43934  0.28488  0.1432  -0.11852 -0.27219]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 's', Embedding: [ 0.18209  0.88297 -0.49805  0.53137 -0.36084]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 's', Embedding: [ 0.18209  0.88297 -0.49805  0.53137 -0.36084]\n",
            "Token: 'a', Embedding: [ 0.24169 -0.34534 -0.22307 -1.2907   0.25285]\n",
            "Token: 'whole', Embedding: [ 0.24418 -0.18986  0.50628  0.50667 -0.21567]\n",
            "Token: 'life', Embedding: [ 0.34098   0.41888  -0.31878   0.031399  0.047223]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'that', Embedding: [ 0.14805   0.10875  -0.036278 -0.15386   0.37181 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'knowing', Embedding: [ 0.35481 -0.05835  0.36272 -0.12141 -0.53815]\n",
            "Token: 'that', Embedding: [ 0.14805   0.10875  -0.036278 -0.15386   0.37181 ]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'sun', Embedding: [ 0.45253  0.35506  0.3146  -0.20477  0.66384]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We see that the nltk considers the punctuation marks as tokens\n",
        "and Glove only has vectors corrosponding to words and not the pinctuation marks, and eventough we have asked to return a 0 vector for the words not in the glove embeddings we are getting a specific vector value**"
      ],
      "metadata": {
        "id": "-RoW4SM5XIZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to remove the punctuations first to get proper tokens"
      ],
      "metadata": {
        "id": "qend_ml1Zzal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the data\n",
        "cleaned_data = []\n",
        "for sentence in data:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokens = [w for w in tokens if w not in string.punctuation]\n",
        "    cleaned_data.append(\" \".join(tokens))"
      ],
      "metadata": {
        "id": "JyxXRtnVaoip"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = [sentence.lower() for sentence in cleaned_data]\n"
      ],
      "metadata": {
        "id": "7b5yZ8H_bins"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing cleaned data to tokeniser\n",
        "# Tokenize the input data\n",
        "tokens = word_tokenize(cleaned_data)\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "VLEsIJQBbNip",
        "outputId": "a73a6a46-14f7-4d29-8ccc-5e9a690426a9"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1485110899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Passing cleaned data to tokeniser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Tokenize the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tokens: {tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[1;32m    119\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \"\"\"\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \"\"\"\n\u001b[1;32m   1456\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mprevious_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mprevious_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m             \u001b[0;31m# Get the slice of the previous word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m             \u001b[0mbefore_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprevious_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we're getting an issue because word_tokenize() expects a single string, but we are passing a list of sentences (cleaned_data)."
      ],
      "metadata": {
        "id": "uENnv-5Fcy_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To get single list of all tokens\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in cleaned_data:\n",
        "    all_tokens.extend(word_tokenize(sentence))\n",
        "\n",
        "print(all_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRu5Y3vOdKMi",
        "outputId": "e7e6f18a-e3ba-4e44-c27b-7a369078323b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['y', 'e', 's', 'l', 'i', 'f', 'e', 'i', 's', 'f', 'u', 'l', 'l', 't', 'h', 'e', 'r', 'e', 'i', 's', 'l', 'i', 'f', 'e', 'e', 'v', 'e', 'n', 'u', 'n', 'd', 'e', 'r', 'g', 'r', 'o', 'u', 'n', 'd', '”', 'h', 'e', 'b', 'e', 'g', 'a', 'n', 'a', 'g', 'a', 'i', 'n', '“', 'y', 'o', 'u', 'w', 'o', 'u', 'l', 'd', 'n', '’', 't', 'b', 'e', 'l', 'i', 'e', 'v', 'e', 'a', 'l', 'e', 'x', 'e', 'y', 'h', 'o', 'w', 'i', 'w', 'a', 'n', 't', 't', 'o', 'l', 'i', 'v', 'e', 'n', 'o', 'w', 'w', 'h', 'a', 't', 'a', 't', 'h', 'i', 'r', 's', 't', 'f', 'o', 'r', 'e', 'x', 'i', 's', 't', 'e', 'n', 'c', 'e', 'a', 'n', 'd', 'c', 'o', 'n', 's', 'c', 'i', 'o', 'u', 's', 'n', 'e', 's', 's', 'h', 'a', 's', 's', 'p', 'r', 'u', 'n', 'g', 'u', 'p', 'i', 'n', 'm', 'e', 'w', 'i', 't', 'h', 'i', 'n', 't', 'h', 'e', 's', 'e', 'p', 'e', 'e', 'l', 'i', 'n', 'g', 'w', 'a', 'l', 'l', 's', '…', 'a', 'n', 'd', 'w', 'h', 'a', 't', 'i', 's', 's', 'u', 'f', 'f', 'e', 'r', 'i', 'n', 'g', 'i', 'a', 'm', 'n', 'o', 't', 'a', 'f', 'r', 'a', 'i', 'd', 'o', 'f', 'i', 't', 'e', 'v', 'e', 'n', 'i', 'f', 'i', 't', 'w', 'e', 'r', 'e', 'b', 'e', 'y', 'o', 'n', 'd', 'r', 'e', 'c', 'k', 'o', 'n', 'i', 'n', 'g', 'i', 'a', 'm', 'n', 'o', 't', 'a', 'f', 'r', 'a', 'i', 'd', 'o', 'f', 'i', 't', 'n', 'o', 'w', 'i', 'w', 'a', 's', 'a', 'f', 'r', 'a', 'i', 'd', 'o', 'f', 'i', 't', 'b', 'e', 'f', 'o', 'r', 'e', '…', 'a', 'n', 'd', 'i', 's', 'e', 'e', 'm', 't', 'o', 'h', 'a', 'v', 'e', 's', 'u', 'c', 'h', 's', 't', 'r', 'e', 'n', 'g', 't', 'h', 'i', 'n', 'm', 'e', 'n', 'o', 'w', 't', 'h', 'a', 't', 'i', 't', 'h', 'i', 'n', 'k', 'i', 'c', 'o', 'u', 'l', 'd', 's', 't', 'a', 'n', 'd', 'a', 'n', 'y', 't', 'h', 'i', 'n', 'g', 'a', 'n', 'y', 's', 'u', 'f', 'f', 'e', 'r', 'i', 'n', 'g', 'o', 'n', 'l', 'y', 't', 'o', 'b', 'e', 'a', 'b', 'l', 'e', 't', 'o', 's', 'a', 'y', 'a', 'n', 'd', 't', 'o', 'r', 'e', 'p', 'e', 'a', 't', 't', 'o', 'm', 'y', 's', 'e', 'l', 'f', 'e', 'v', 'e', 'r', 'y', 'm', 'o', 'm', 'e', 'n', 't', '‘', 'i', 'e', 'x', 'i', 's', 't', '’', 'i', 'n', 't', 'h', 'o', 'u', 's', 'a', 'n', 'd', 's', 'o', 'f', 'a', 'g', 'o', 'n', 'i', 'e', 's', '—', 'i', 'e', 'x', 'i', 's', 't', 'i', '’', 'm', 't', 'o', 'r', 'm', 'e', 'n', 't', 'e', 'd', 'o', 'n', 't', 'h', 'e', 'r', 'a', 'c', 'k', '—', 'b', 'u', 't', 'i', 'e', 'x', 'i', 's', 't', 't', 'h', 'o', 'u', 'g', 'h', 'i', 's', 'i', 't', 'a', 'l', 'o', 'n', 'e', 'o', 'n', 'a', 'p', 'i', 'l', 'l', 'a', 'r', '—', 'i', 'e', 'x', 'i', 's', 't', 'i', 's', 'e', 'e', 't', 'h', 'e', 's', 'u', 'n', 'a', 'n', 'd', 'i', 'f', 'i', 'd', 'o', 'n', '’', 't', 's', 'e', 'e', 't', 'h', 'e', 's', 'u', 'n', 'i', 'k', 'n', 'o', 'w', 'i', 't', '’', 's', 't', 'h', 'e', 'r', 'e', 'a', 'n', 'd', 't', 'h', 'e', 'r', 'e', '’', 's', 'a', 'w', 'h', 'o', 'l', 'e', 'l', 'i', 'f', 'e', 'i', 'n', 't', 'h', 'a', 't', 'i', 'n', 'k', 'n', 'o', 'w', 'i', 'n', 'g', 't', 'h', 'a', 't', 't', 'h', 'e', 's', 'u', 'n', 'i', 's', 't', 'h', 'e', 'r', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple method to get the tokens from list: cleaned_data is to use Keras tokeniser."
      ],
      "metadata": {
        "id": "vCJ5C2Qtdfq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)   # data = list of sentences\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "2ACAW-raVozb"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the tokens with their index\n",
        "for word, index in word_index.items():\n",
        "    print(word, \"→\", index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVU11BxMeN5d",
        "outputId": "ef8acd5a-e32d-4396-9f97-f5839caef623"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e → 1\n",
            "i → 2\n",
            "n → 3\n",
            "t → 4\n",
            "a → 5\n",
            "s → 6\n",
            "o → 7\n",
            "h → 8\n",
            "r → 9\n",
            "l → 10\n",
            "f → 11\n",
            "u → 12\n",
            "d → 13\n",
            "w → 14\n",
            "g → 15\n",
            "y → 16\n",
            "m → 17\n",
            "b → 18\n",
            "c → 19\n",
            "v → 20\n",
            "’ → 21\n",
            "x → 22\n",
            "p → 23\n",
            "k → 24\n",
            "— → 25\n",
            "… → 26\n",
            "” → 27\n",
            "“ → 28\n",
            "‘ → 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple and safe practice to print the tokens"
      ],
      "metadata": {
        "id": "_JlWn_0geuPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(cleaned_data):\n",
        "    print(\"Sentence:\", sent)\n",
        "    print(\"Tokens:\", word_tokenize(sent))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iB_ycQY8e3oY",
        "outputId": "60b58fa4-27e1-4831-d724-bdeb53896fc1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: v\n",
            "Tokens: ['v']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: ”\n",
            "Tokens: ['”']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: b\n",
            "Tokens: ['b']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: “\n",
            "Tokens: ['“']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: ’\n",
            "Tokens: ['’']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: b\n",
            "Tokens: ['b']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: v\n",
            "Tokens: ['v']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: x\n",
            "Tokens: ['x']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: v\n",
            "Tokens: ['v']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: x\n",
            "Tokens: ['x']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: c\n",
            "Tokens: ['c']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: c\n",
            "Tokens: ['c']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: c\n",
            "Tokens: ['c']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: p\n",
            "Tokens: ['p']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: p\n",
            "Tokens: ['p']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: p\n",
            "Tokens: ['p']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: …\n",
            "Tokens: ['…']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: v\n",
            "Tokens: ['v']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: b\n",
            "Tokens: ['b']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: c\n",
            "Tokens: ['c']\n",
            "\n",
            "Sentence: k\n",
            "Tokens: ['k']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: b\n",
            "Tokens: ['b']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: …\n",
            "Tokens: ['…']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: v\n",
            "Tokens: ['v']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: c\n",
            "Tokens: ['c']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: k\n",
            "Tokens: ['k']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: c\n",
            "Tokens: ['c']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: b\n",
            "Tokens: ['b']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: b\n",
            "Tokens: ['b']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: p\n",
            "Tokens: ['p']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: v\n",
            "Tokens: ['v']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: y\n",
            "Tokens: ['y']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: ‘\n",
            "Tokens: ['‘']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: x\n",
            "Tokens: ['x']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: ’\n",
            "Tokens: ['’']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: —\n",
            "Tokens: ['—']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: x\n",
            "Tokens: ['x']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: ’\n",
            "Tokens: ['’']\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: m\n",
            "Tokens: ['m']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: c\n",
            "Tokens: ['c']\n",
            "\n",
            "Sentence: k\n",
            "Tokens: ['k']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: —\n",
            "Tokens: ['—']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: b\n",
            "Tokens: ['b']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: x\n",
            "Tokens: ['x']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: p\n",
            "Tokens: ['p']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: —\n",
            "Tokens: ['—']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: x\n",
            "Tokens: ['x']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: ’\n",
            "Tokens: ['’']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: k\n",
            "Tokens: ['k']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: ’\n",
            "Tokens: ['’']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: d\n",
            "Tokens: ['d']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: ’\n",
            "Tokens: ['’']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: l\n",
            "Tokens: ['l']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: f\n",
            "Tokens: ['f']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: k\n",
            "Tokens: ['k']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: o\n",
            "Tokens: ['o']\n",
            "\n",
            "Sentence: w\n",
            "Tokens: ['w']\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: g\n",
            "Tokens: ['g']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: a\n",
            "Tokens: ['a']\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: u\n",
            "Tokens: ['u']\n",
            "\n",
            "Sentence: n\n",
            "Tokens: ['n']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: i\n",
            "Tokens: ['i']\n",
            "\n",
            "Sentence: s\n",
            "Tokens: ['s']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n",
            "Sentence: t\n",
            "Tokens: ['t']\n",
            "\n",
            "Sentence: h\n",
            "Tokens: ['h']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: r\n",
            "Tokens: ['r']\n",
            "\n",
            "Sentence: e\n",
            "Tokens: ['e']\n",
            "\n",
            "Sentence: \n",
            "Tokens: []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "3ckhFDa2ETiy"
      },
      "outputs": [],
      "source": [
        "# Mapping words to GloVe embeddings\n",
        "def get_embedding_matrix(word_index, glove_embeddings, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = glove_embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 200\n",
        "embedding_matrix = get_embedding_matrix(word_index, glove_embeddings, embedding_dim)\n"
      ],
      "metadata": {
        "id": "zWUHmtB7VAe9"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first 5 embeddings for the first 5 tokens of the data\n",
        "print(embeddings_array[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dLtlYApMXEWA",
        "outputId": "b2c73816-5b6c-4c81-a67e-a18109823a5d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 0.22042  ,  0.24924  ,  0.17393  , -0.016275 , -0.30916  ,\n",
            "        0.2661   , -0.74482  ,  0.083853 ,  0.32016  ,  0.65026  ,\n",
            "       -0.30558  ,  0.016842 ,  0.048508 , -0.29494  , -0.78175  ,\n",
            "        0.41424  , -0.2149   ,  0.53132  ,  0.48179  ,  0.15397  ,\n",
            "        0.30971  ,  1.4896   , -0.08224  ,  0.32086  ,  0.028866 ,\n",
            "       -0.29613  , -0.13558  , -0.074538 ,  0.23826  , -0.53784  ,\n",
            "       -0.41572  , -0.13477  ,  0.15444  , -0.12929  , -0.34317  ,\n",
            "       -0.15537  ,  0.033685 , -0.35089  , -0.22403  ,  0.49218  ,\n",
            "       -0.1475   ,  0.0034514, -0.24207  , -0.10924  ,  0.0057027,\n",
            "        0.12135  ,  0.60403  , -0.17868  , -0.39604  ,  0.056209 ,\n",
            "        0.51824  , -0.39824  ,  0.13078  ,  0.13691  , -0.15311  ,\n",
            "       -0.049687 ,  0.049536 , -0.033107 , -0.012196 , -0.32973  ,\n",
            "        0.25021  , -0.32097  ,  0.14448  ,  0.05555  ,  0.061817 ,\n",
            "       -0.075844 , -0.19434  ,  0.62882  ,  0.83724  ,  0.11561  ,\n",
            "        0.19471  ,  0.003975 ,  0.017576 , -0.36561  , -0.41066  ,\n",
            "       -0.47927  , -0.52158  ,  0.21736  , -0.56578  , -0.15558  ,\n",
            "        0.35471  , -0.0070188, -0.0019452,  0.17939  , -0.35319  ,\n",
            "       -0.25795  , -0.31974  , -0.044591 ,  0.12496  , -0.93906  ,\n",
            "       -0.14558  , -0.23784  ,  0.33792  , -0.25189  , -0.22839  ,\n",
            "        0.078948 ,  0.35851  ,  0.021203 , -0.81422  ,  0.2317   ,\n",
            "        0.47906  ,  0.096946 ,  0.23977  , -0.38182  ,  0.37931  ,\n",
            "       -0.036341 , -0.36294  ,  1.3091   , -0.04805  ,  0.3774   ,\n",
            "        0.014168 ,  0.12427  ,  0.080578 ,  0.02179  , -0.57981  ,\n",
            "        0.019524 , -0.081258 , -0.021899 , -0.28081  ,  0.21055  ,\n",
            "        0.19828  , -0.083682 ,  0.14192  ,  0.068194 ,  0.20974  ,\n",
            "       -0.45658  , -0.023901 , -0.6173   ,  0.4895   , -0.30713  ,\n",
            "       -0.15431  , -0.19995  , -0.036085 ,  0.17406  , -0.0072787,\n",
            "       -0.10505  , -0.14231  , -0.21916  ,  0.1786   , -0.027257 ,\n",
            "        0.10534  ,  0.073802 ,  0.1293   , -0.41754  ,  1.2693   ,\n",
            "        0.14427  ,  0.37295  , -0.42493  , -0.69197  ,  0.08078  ,\n",
            "        0.014119 ,  0.67835  ,  0.16693  ,  0.26872  ,  0.32128  ,\n",
            "        0.0046269, -0.02123  , -0.13293  ,  0.14867  ,  0.036675 ,\n",
            "       -0.098082 , -0.24252  , -0.055073 , -0.33584  , -0.32442  ,\n",
            "       -0.5476   ,  0.21091  ,  0.12983  , -0.1429   , -0.6026   ,\n",
            "       -0.076548 ,  0.4574   ,  0.27267  ,  0.12926  , -0.2167   ,\n",
            "       -0.15686  , -0.46502  ,  0.15973  , -0.15449  ,  0.3639   ,\n",
            "        0.97274  ,  0.11432  , -0.033772 ,  0.023939 , -0.23805  ,\n",
            "       -0.31564  ,  0.5012   ,  0.049548 ,  0.20645  , -0.66727  ,\n",
            "       -0.2323   ,  0.11915  ,  0.22961  , -0.41109  ,  0.050235 ,\n",
            "       -0.38869  , -0.096837 , -0.088502 , -0.19784  ,  0.3889   ],\n",
            "      dtype=float32), array([ 1.7651e-01,  2.9208e-01, -2.0768e-03, -3.7523e-01,  4.9139e-03,\n",
            "        2.3979e-01, -2.8893e-01, -1.4643e-02, -1.0993e-01,  1.5592e-01,\n",
            "        2.0627e-01,  4.7675e-01,  9.9907e-02, -1.4058e-01,  2.1114e-01,\n",
            "        1.2126e-01, -3.1831e-01, -8.9433e-02, -9.0553e-02, -3.1962e-01,\n",
            "        2.1319e-01,  2.4844e+00, -7.7521e-02, -8.4279e-02,  2.0186e-01,\n",
            "        2.6084e-01, -4.0411e-01, -1.9127e-01,  2.4715e-01,  2.2394e-01,\n",
            "       -6.3437e-02,  2.0379e-01, -1.8463e-01, -8.8413e-02,  2.4169e-02,\n",
            "       -2.8769e-01, -6.1246e-01, -1.2683e-01, -8.8273e-02,  1.8331e-01,\n",
            "       -5.3161e-01, -1.9970e-01, -2.6703e-01,  1.5312e-01, -1.5239e-02,\n",
            "       -8.2844e-02,  4.7856e-01, -2.9612e-01,  1.1168e-01, -2.5790e-02,\n",
            "       -1.1697e-02,  1.9923e-01, -1.4267e-01,  6.6250e-01, -5.1739e-02,\n",
            "       -1.6938e-01, -1.5635e-01,  9.2806e-02,  3.2548e-01,  1.1724e-01,\n",
            "        2.8788e-01, -6.0651e-02, -1.4153e-01,  1.6668e-01,  2.6861e-01,\n",
            "       -3.1001e-02, -3.9665e-01,  3.5304e-01,  2.3850e-01,  1.2388e-01,\n",
            "        4.5698e-01, -1.2559e-01, -1.2804e-01,  3.7449e-01,  2.4460e-01,\n",
            "        2.3073e-01,  2.0808e-01,  5.1258e-02, -2.1816e-01, -3.6409e-02,\n",
            "       -3.8800e-02, -4.2487e-02, -3.0779e-01, -2.5449e-02,  2.2532e-01,\n",
            "        4.5538e-02, -4.8934e-01, -1.3988e-01,  1.7394e-01, -4.6137e-01,\n",
            "       -2.6555e-01,  1.5473e-01,  6.3816e-02, -1.7022e-01, -1.5762e-01,\n",
            "        7.5765e-02,  1.2151e-01, -4.9340e-01, -1.0909e-01,  3.4487e-02,\n",
            "        2.9947e-01,  1.8690e-02, -1.6534e-01,  1.6679e-02,  1.6341e-01,\n",
            "       -2.7418e-01,  7.7797e-02,  1.4023e+00,  2.5275e-02,  9.4725e-02,\n",
            "       -4.0735e-02, -1.0642e-01,  2.3364e-02,  7.9143e-02, -1.6615e-01,\n",
            "       -2.3013e-01, -1.4071e-01,  4.0159e-01, -3.4951e-01,  1.8545e-02,\n",
            "        2.2434e-01,  7.6922e-01,  2.4722e-01,  1.4936e-01,  4.2368e-01,\n",
            "       -7.2059e-01, -3.8541e-02,  1.5522e-01,  3.3596e-01, -4.3077e-01,\n",
            "       -2.6925e-02, -3.7733e-01,  2.4271e-01, -4.6495e-01,  4.5783e-01,\n",
            "        2.3693e-01,  7.9361e-02, -3.2244e-01, -4.2434e-01, -1.1138e-01,\n",
            "        5.5426e-01,  8.5153e-02, -2.0581e-02, -4.6386e-02,  1.2467e+00,\n",
            "        1.3177e-01,  6.7092e-02, -5.7780e-01,  1.3586e-02, -7.1274e-02,\n",
            "        1.7311e-02,  8.9781e-02,  1.9857e-01, -3.2205e-02,  6.4843e-01,\n",
            "       -2.3797e-01, -1.9676e-01,  2.0203e-01,  2.1074e-01, -5.0347e-01,\n",
            "        2.6823e-02, -4.5444e-02, -2.2642e-01, -1.9977e-01, -1.2138e-01,\n",
            "        1.6941e-01,  6.1998e-02,  4.2631e-01, -8.8383e-02,  4.5756e-01,\n",
            "        7.7774e-02,  6.1342e-02,  4.5710e-01, -1.7787e-01, -1.4597e-01,\n",
            "        3.2654e-01,  2.4430e-03, -1.1886e-01,  1.0081e-01, -2.0011e-02,\n",
            "        1.0366e+00, -3.9814e-01, -6.8180e-01,  2.3685e-01, -2.0396e-01,\n",
            "       -1.7668e-01, -3.1385e-01,  1.4834e-01, -5.2187e-02,  6.1300e-02,\n",
            "       -3.2582e-01,  1.9153e-01, -1.5469e-01, -1.4679e-01,  4.6971e-02,\n",
            "        3.2325e-02, -2.2006e-01, -2.0774e-01, -2.3189e-01, -1.0814e-01],\n",
            "      dtype=float32), array([ 3.4098e-01,  4.1888e-01, -3.1878e-01,  3.1399e-02,  4.7223e-02,\n",
            "        2.4906e-01, -7.1631e-01,  5.2904e-02, -3.4208e-03,  2.1647e-01,\n",
            "       -3.0463e-02,  8.9968e-01,  3.5829e-01, -6.6277e-02,  8.1989e-02,\n",
            "        3.9360e-01,  9.9335e-02,  7.3747e-01,  3.2178e-01, -2.2824e-01,\n",
            "        3.9632e-01,  2.9324e+00,  2.0929e-01, -2.6525e-01,  1.2720e-01,\n",
            "        8.7867e-02, -3.5642e-03, -5.1395e-02, -3.8543e-01,  2.6079e-01,\n",
            "        5.8987e-02, -3.2565e-02,  3.4674e-01, -6.5351e-02, -1.2305e-01,\n",
            "        1.8915e-02, -1.9741e-01,  2.4483e-01, -7.6178e-02,  1.4100e-01,\n",
            "       -9.6150e-01, -1.7128e-02, -1.9702e-01,  5.8651e-01, -1.0661e-01,\n",
            "        1.0472e-01,  6.7752e-01,  4.9478e-01,  1.1170e-01,  1.8718e-02,\n",
            "       -2.6009e-01,  4.9720e-01, -1.5728e-01,  6.0994e-01,  1.8953e-01,\n",
            "        1.1996e-01,  2.7827e-01, -1.5831e-02,  3.9711e-02, -3.7136e-01,\n",
            "       -2.8909e-01, -3.2909e-01, -7.1628e-01,  5.6670e-01, -2.3132e-01,\n",
            "       -1.4021e-01,  5.5793e-01,  3.1357e-01, -5.7165e-01,  3.4597e-01,\n",
            "        7.1454e-01, -1.7147e-01,  8.9315e-02,  8.1979e-01,  2.3206e-01,\n",
            "        1.9429e-01, -4.4881e-01, -2.8906e-01, -1.5707e-02,  1.4476e-01,\n",
            "       -1.4431e-02,  1.1904e-01, -6.6498e-01, -3.8981e-02,  1.7994e-01,\n",
            "        9.7028e-02, -3.4582e-01,  6.7696e-03,  4.7810e-01, -6.0951e-01,\n",
            "        6.4572e-01,  7.3228e-04,  3.7128e-01,  1.7062e-01, -2.6385e-01,\n",
            "       -4.2804e-01, -2.2658e-01, -1.8425e-01,  5.2100e-03,  1.8815e-01,\n",
            "        1.2560e-01,  6.0294e-02, -2.0199e-01,  2.6285e-02,  2.4987e-02,\n",
            "       -7.4668e-01,  2.0404e-01,  7.0431e-01, -2.0175e-01, -2.1715e-01,\n",
            "        1.6100e-01, -1.3599e-01,  1.9623e-01,  2.2066e-02, -3.3890e-01,\n",
            "       -1.9406e-01, -1.5501e-01,  5.4023e-01, -6.1807e-01, -3.9910e-01,\n",
            "       -1.7745e-01,  2.6452e-01, -1.2936e-01, -1.9009e-01, -8.3096e-02,\n",
            "       -3.2137e-01,  5.8294e-01,  1.6789e-01,  4.7591e-02, -9.2068e-01,\n",
            "       -3.5928e-01, -1.0102e-01,  9.4616e-02,  5.1867e-02,  8.9359e-02,\n",
            "        1.6542e-01,  7.5411e-01, -1.1097e-01, -2.3721e-01, -1.3093e-01,\n",
            "        5.3672e-01,  3.0311e-01, -8.0717e-02, -1.6100e-01,  1.4385e+00,\n",
            "        1.9252e-02, -1.5350e-01,  1.8693e-02,  2.7981e-01,  4.8011e-01,\n",
            "        7.4099e-02, -1.0303e-01, -2.4328e-01,  6.9507e-02,  3.4220e-01,\n",
            "       -1.9133e-01,  5.9109e-02, -2.8913e-02, -1.6200e-01,  3.7345e-01,\n",
            "        1.4532e-01,  7.4352e-01,  9.0516e-02, -3.8116e-01,  1.1042e-01,\n",
            "       -2.5854e-01, -4.0525e-01, -3.6723e-02, -1.8737e-01,  3.2509e-01,\n",
            "       -2.7669e-01,  3.1600e-01,  3.0019e-01,  5.3721e-01, -4.3686e-01,\n",
            "        1.5107e-01, -2.3496e-01, -9.7297e-02,  4.1126e-01,  6.4296e-01,\n",
            "        1.2471e+00, -1.7064e-01, -3.7883e-01,  3.1271e-02, -1.9095e-01,\n",
            "        5.0932e-01,  4.7580e-02, -9.3402e-02,  6.1584e-01,  2.1330e-01,\n",
            "       -2.4400e-01,  5.7573e-02, -1.7811e-01, -1.7605e-01,  1.4003e-01,\n",
            "       -6.9920e-01,  2.8465e-01,  7.2919e-01,  3.8914e-01,  3.7687e-01],\n",
            "      dtype=float32), array([ 0.32928  ,  0.25526  ,  0.26753  , -0.084809 ,  0.29764  ,\n",
            "        0.062339 , -0.15475  ,  0.17784  ,  0.32328  , -0.92752  ,\n",
            "        0.15194  ,  0.16324  , -0.10428  , -0.026464 ,  0.65971  ,\n",
            "        0.14782  ,  0.38623  ,  0.25169  ,  0.1261   , -0.43138  ,\n",
            "        0.28092  ,  3.1604   , -0.17565  , -0.0032247,  0.64389  ,\n",
            "       -0.39697  ,  0.18975  ,  0.37999  , -0.079175 , -0.14781  ,\n",
            "       -0.072965 ,  0.057247 , -0.42314  ,  0.4508   , -0.097386 ,\n",
            "       -0.47587  , -0.96599  , -0.75595  , -0.033932 , -0.070886 ,\n",
            "       -0.44828  , -0.52094  , -0.1823   ,  0.18582  , -0.074273 ,\n",
            "       -0.017871 ,  0.16742  ,  0.015459 ,  0.3029   , -0.1258   ,\n",
            "        0.32418  , -0.31263  , -0.076832 ,  0.051959 ,  0.27242  ,\n",
            "       -0.18285  , -0.36479  , -0.63562  , -0.21685  ,  0.035812 ,\n",
            "        0.12485  ,  0.37268  , -0.16976  , -0.094146 , -0.16412  ,\n",
            "       -0.10728  ,  0.037866 ,  0.1175   , -0.15533  ,  0.34062  ,\n",
            "        0.58848  ,  0.38992  , -0.54839  ,  0.85013  , -0.83728  ,\n",
            "        0.15482  , -0.37191  , -0.65409  , -0.27631  , -0.025224 ,\n",
            "        0.075732 , -0.23904  , -0.18311  , -0.084571 ,  0.15492  ,\n",
            "       -0.16317  , -0.26499  ,  0.056831 ,  0.88287  , -0.47655  ,\n",
            "        0.25131  , -0.09316  ,  0.34377  , -0.35863  , -0.22855  ,\n",
            "        0.11918  ,  0.29661  , -0.2536   ,  0.049002 , -0.21234  ,\n",
            "        0.16237  ,  0.53871  ,  0.035344 ,  0.39293  , -0.29673  ,\n",
            "       -0.72556  , -0.27431  ,  1.3469   , -0.19218  ,  0.50534  ,\n",
            "        0.028451 , -0.32206  ,  0.096035 , -0.0083551, -0.013107 ,\n",
            "       -0.32444  , -0.10163  ,  0.031755 , -0.63196  , -0.21541  ,\n",
            "       -0.035609 ,  0.31259  ,  0.23988  , -0.19056  , -0.13086  ,\n",
            "       -0.12644  ,  0.48795  , -0.13492  , -0.41967  ,  0.15904  ,\n",
            "       -0.27921  , -0.017258 ,  0.2937   ,  0.067436 ,  0.085052 ,\n",
            "        0.099394 , -0.0055281,  0.094985 ,  0.11167  ,  0.19749  ,\n",
            "        0.2523   ,  0.32205  ,  0.42778  , -0.03518  ,  1.3291   ,\n",
            "        0.005261 ,  0.26769  , -0.46168  ,  0.1125   ,  0.10111  ,\n",
            "       -0.31174  ,  0.5458   , -0.37363  , -0.026133 ,  0.99566  ,\n",
            "       -0.15827  , -0.26202  ,  0.17324  ,  0.060104 , -0.48004  ,\n",
            "        0.23841  , -0.21495  ,  0.077693 , -0.089078 ,  0.12985  ,\n",
            "       -0.174    , -0.057151 ,  0.48207  , -0.14668  ,  0.26739  ,\n",
            "       -0.33366  ,  0.32552  ,  0.6252   , -0.30905  ,  0.087737 ,\n",
            "       -0.17204  ,  0.28246  , -0.037268 ,  0.16007  ,  0.30031  ,\n",
            "        1.4061   , -0.32169  , -0.025792 ,  0.037175 ,  0.026222 ,\n",
            "       -0.27671  ,  0.051688 , -0.058734 , -0.23223  , -0.10529  ,\n",
            "       -0.40318  , -0.22161  ,  0.060587 ,  0.091321 , -0.21363  ,\n",
            "        0.071634 , -0.21331  ,  0.074621 ,  0.012001 , -0.21952  ],\n",
            "      dtype=float32), array([ 2.9667e-01, -1.9041e-01, -1.6734e-01, -2.3067e-01,  3.5460e-02,\n",
            "       -1.5565e-01,  1.0894e-01, -1.8320e-01, -7.8841e-02,  3.1029e-01,\n",
            "       -2.6727e-01,  2.6272e-01,  2.6124e-01, -3.3728e-01,  4.2254e-01,\n",
            "       -4.8555e-01,  1.3919e-01,  3.5297e-01, -1.9247e-01, -2.2333e-01,\n",
            "        4.9861e-01,  2.7357e+00,  1.3011e-01, -5.0954e-01,  4.5014e-01,\n",
            "       -8.1410e-02, -2.3534e-01,  1.0371e-02,  4.3054e-01, -2.1223e-01,\n",
            "       -1.9638e-01, -3.1434e-01, -1.2297e-01, -5.6133e-01, -9.0384e-02,\n",
            "       -3.3263e-02, -3.2137e-01, -5.4950e-01, -1.0027e-02,  2.3146e-01,\n",
            "        1.0421e-02, -1.3659e-02, -7.6713e-03, -1.3176e-01,  1.0317e-02,\n",
            "        3.7078e-01,  4.6923e-01,  5.9423e-02,  4.1123e-01,  1.0023e-02,\n",
            "       -9.0452e-02, -4.0759e-01, -1.1723e-01,  3.0375e-01,  2.5175e-01,\n",
            "        1.5379e-01,  1.5170e-01, -2.0262e-02, -1.7740e-01, -1.8909e-01,\n",
            "        6.7966e-01, -6.6682e-01, -4.9400e-01, -5.9751e-01,  1.0683e-01,\n",
            "        1.3225e-01,  2.7032e-01, -1.0069e-01,  2.5423e-01,  1.3450e-01,\n",
            "        5.0991e-01,  2.7086e-01,  3.4327e-01,  2.3083e-02, -1.2101e-01,\n",
            "        1.7861e-01, -1.9951e-01, -2.3792e-01,  4.3102e-01,  5.5752e-02,\n",
            "        1.6033e-01, -3.4647e-01, -2.1565e-01, -7.6750e-02,  1.7376e-01,\n",
            "       -2.6902e-01,  2.2254e-01, -3.2923e-01,  7.6815e-01, -7.1194e-01,\n",
            "        3.4376e-01,  2.2850e-01,  4.1817e-01,  1.3746e-02,  2.8038e-01,\n",
            "       -3.1916e-01,  3.1060e-01,  2.9356e-01, -2.8953e-01,  1.0565e-01,\n",
            "        2.7560e-01, -3.7460e-01,  6.0083e-01,  1.7285e-04, -2.1326e-01,\n",
            "        3.0623e-02,  9.0689e-02,  1.0639e+00, -4.1776e-01, -2.2014e-01,\n",
            "       -3.7017e-01, -4.8046e-01,  6.7010e-02, -7.5316e-02,  8.5481e-02,\n",
            "       -4.0933e-01,  2.6082e-01, -2.5018e-01,  9.1720e-02, -1.2541e-01,\n",
            "       -4.6252e-01,  2.0970e-01, -2.3605e-01,  2.8665e-01,  3.5655e-02,\n",
            "       -3.4872e-01,  4.5211e-01, -1.3086e-01,  3.5587e-01,  6.8709e-02,\n",
            "       -4.8611e-02, -7.7557e-03,  9.3124e-02, -1.6243e-01,  3.5604e-01,\n",
            "        2.6036e-01,  1.2996e-01, -5.3751e-01,  5.2925e-01,  1.7803e-01,\n",
            "        4.2875e-01, -3.2468e-01,  2.8189e-01,  3.7518e-01,  1.2964e+00,\n",
            "        3.4064e-02, -3.0608e-01, -1.6244e-01, -1.8960e-01, -1.9614e-01,\n",
            "        3.5166e-01, -2.4670e-01, -2.1728e-01,  1.9992e-01,  2.7304e-02,\n",
            "       -1.4604e-01, -3.4660e-01, -1.2907e-02,  6.7934e-02, -3.3142e-01,\n",
            "        2.1655e-01, -2.3391e-01,  2.1805e-01,  4.7136e-02,  1.8012e-01,\n",
            "        1.2672e-01, -5.3111e-01, -4.8423e-01, -5.1849e-01,  6.2180e-01,\n",
            "        2.8892e-02,  6.2254e-02,  2.1884e-01,  4.6848e-01, -3.4412e-02,\n",
            "        1.1361e-01, -6.7913e-01, -3.8568e-01, -3.0395e-01, -4.9260e-02,\n",
            "        1.3075e+00, -5.2156e-01, -3.1272e-01, -2.5363e-01, -1.0971e-01,\n",
            "       -2.0867e-01, -4.4347e-01,  1.1496e-01,  1.4502e-01,  3.9719e-01,\n",
            "       -1.6284e-01,  3.0485e-01, -1.3438e-01, -6.4691e-01,  1.1908e-01,\n",
            "       -5.3820e-01,  5.5905e-01, -4.4982e-02, -5.2480e-01,  4.3902e-01],\n",
            "      dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus we have sucessfully generated Glove ebeddings for out input data"
      ],
      "metadata": {
        "id": "_CVN1FWkh_dx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4FPdQk78Wpyvd+RXhSXpK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}